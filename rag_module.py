"""
RAG ëª¨ë“ˆ: ì‚¬ì—…ê³µê³  ë¬¸ì„œë¥¼ FAISS ë²¡í„° DBë¡œ êµ¬ì¶•í•˜ê³  ê²€ìƒ‰/ì§ˆì˜ì‘ë‹µ ìˆ˜í–‰
- PDF, HWP(í•œê¸€) íŒŒì¼ ìžë™ ê°ì§€ ë° ì²˜ë¦¬
- ê°œì„ ëœ ì§ˆì˜ì‘ë‹µ ê¸°ëŠ¥ (Top-k ê²€ìƒ‰ + ìžì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„±)
- GPT-5 temperature=1 ê°•ì œ ì„¤ì • (Langchain í•¨ìˆ˜ ì´ìŠˆ)
- LLM ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì§ˆë¬¸ ìƒì„± (ë„ì„œ ë‚´ìš© ì´í•´í•˜ê³  ì§ˆë¬¸ ìƒì„±)
"""
import os
import tempfile
from pathlib import Path
from typing import List, Optional, Tuple
import requests
import streamlit as st
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_community.document_loaders import (
    PyPDFLoader,
    UnstructuredFileLoader,
    TextLoader
)

class ProjectRAGSystem:
    """
    ì‚¬ì—…ê³µê³ ë³„ RAG ì‹œìŠ¤í…œ
    - ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹± (PDF, HWP, DOCX, TXT)
    - FAISS ë²¡í„° DB êµ¬ì¶•
    - ê°œì„ ëœ ì§ˆì˜ì‘ë‹µ ê¸°ëŠ¥ (Top-k ê²€ìƒ‰ + ìžì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„±)
    - LLM ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì§ˆë¬¸ ìƒì„± (ë„ì„œ ë‚´ìš© ì´í•´í•˜ê³  ì§ˆë¬¸ ìƒì„±)
    """
    
    def __init__(self, openai_api_key: str, llm_base_url: str, embedding_base_url: str = None):
        """
        Args:
            openai_api_key: OpenAI API í‚¤
            llm_base_url: LLM ì—”ë“œí¬ì¸íŠ¸ URL
            embedding_base_url: Embeddings ì—”ë“œí¬ì¸íŠ¸ URL
        """
        print("\n" + "=" * 60)
        print("   ProjectRAGSystem ì´ˆê¸°í™” ì¤‘...")
        print(f"   LLM URL: {llm_base_url}")
        print(f"   Embedding URL (ì „ë‹¬ë°›ìŒ): {embedding_base_url}")
        
        self.api_key = openai_api_key
        self.llm_base_url = llm_base_url
        
        # embedding_base_url ìµœì¢… ì„¤ì •
        if embedding_base_url and embedding_base_url.strip() and embedding_base_url != llm_base_url:
            self.embedding_base_url = embedding_base_url
            print(f"âœ… Embedding URL ì‚¬ìš©: {self.embedding_base_url}")
        else:
            self.embedding_base_url = llm_base_url
            print(f"âŒ Embedding URLì„ LLM URLë¡œ ëŒ€ì²´: {self.embedding_base_url}")
        
        print("=" * 60 + "\n")
        
        # Embeddings ì´ˆê¸°í™” ì‹œë„
        embedding_configs = [
            ("text-embedding-3-small", "base_url"),
            ("text-embedding-ada-002", "base_url"),
            ("openai/text-embedding-3-small", "base_url"),
            ("text-embedding-3-small", "openai_api_base"),
            ("openai/text-embedding-3-small", "openai_api_base"),
        ]
        
        self.embeddings = None
        last_error = None
        
        for model, param_type in embedding_configs:
            try:
                print(f"   ì‹œë„: model={model}, {param_type}={self.embedding_base_url}")
                
                if param_type == "base_url":
                    self.embeddings = OpenAIEmbeddings(
                        openai_api_key=openai_api_key,
                        base_url=self.embedding_base_url,
                        model=model
                    )
                else:  # openai_api_base
                    self.embeddings = OpenAIEmbeddings(
                        openai_api_key=openai_api_key,
                        openai_api_base=self.embedding_base_url,
                        model=model
                    )
                
                # í…ŒìŠ¤íŠ¸ ìž„ë² ë”©
                test_result = self.embeddings.embed_query("í…ŒìŠ¤íŠ¸")
                print(f"âœ… ì„±ê³µ! model={model}, ì°¨ì›={len(test_result)}")
                break
                
            except Exception as e:
                last_error = str(e)
                print(f"âŒ ì‹¤íŒ¨: {last_error[:80]}")
                self.embeddings = None
                continue
        
        if not self.embeddings:
            error_msg = (
                f"\n{'='*60}\n"
                f"âŒ Embeddings ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨\n"
                f"   ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_error}\n"
                f"   Embedding URL: {self.embedding_base_url}\n"
                f"   API Key ì•ž 20ìž: {openai_api_key[:20]}...\n"
                f"{'='*60}\n"
            )
            print(error_msg)
            raise ValueError(error_msg)
        
        # LLM ì´ˆê¸°í™” (GPT-5ëŠ” temperature=1ë§Œ ì§€ì›!)
        print(f"   LLM ì´ˆê¸°í™” ì¤‘... (URL: {llm_base_url})")
        self.llm = ChatOpenAI(
            model="openai/gpt-5",
            openai_api_key=openai_api_key,
            base_url=llm_base_url,
            temperature=1,  # GPT-5 í•„ìˆ˜ ì„¤ì •!
        )
        print(f"âœ… LLM ì´ˆê¸°í™” ì™„ë£Œ (temperature=1 ê°•ì œ ì„¤ì •)\n")
        
        self.vectorstore: Optional[FAISS] = None
    
    def detect_file_type(self, url: str, content: bytes) -> str:
        """
        URLê³¼ íŒŒì¼ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ íŒŒì¼ íƒ€ìž… ê°ì§€
        
        Args:
            url: íŒŒì¼ URL
            content: íŒŒì¼ ë°”ì´ë„ˆë¦¬ ë‚´ìš©
            
        Returns:
            íŒŒì¼ íƒ€ìž… ('pdf', 'hwp', 'docx', 'txt', 'unknown')
        """
        url_lower = url.lower()
        
        # HWP íŒŒì¼ ê°ì§€ (í™•ìž¥ìž ë˜ëŠ” ë§¤ì§ ë„˜ë²„)
        if url_lower.endswith('.hwp') or url_lower.endswith('.hwpx'):
            return 'hwp'
        
        # PDF íŒŒì¼ ê°ì§€
        if url_lower.endswith('.pdf') or content.startswith(b'%PDF'):
            return 'pdf'
        
        # DOCX íŒŒì¼ ê°ì§€
        if url_lower.endswith('.docx') or url_lower.endswith('.doc'):
            return 'docx'
        
        # TXT íŒŒì¼
        if url_lower.endswith('.txt'):
            return 'txt'
        
        # ë§¤ì§ ë„˜ë²„ë¡œ HWP í™•ì¸ (OLE êµ¬ì¡°)
        if content.startswith(b'HWP Document File') or content.startswith(b'\xd0\xcf\x11\xe0'):
            return 'hwp'
        
        # ë§¤ì§ ë„˜ë²„ë¡œ PDF í™•ì¸
        if content.startswith(b'%PDF'):
            return 'pdf'
        
        # DOCXëŠ” ZIP í¬ë§· (PKë¡œ ì‹œìž‘)
        if content.startswith(b'PK\x03\x04'):
            return 'docx'
        
        return 'unknown'
    
    def download_file(self, url: str, save_path: str) -> Tuple[bool, str]:
        """
        URLì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° íƒ€ìž… ê°ì§€
        
        Args:
            url: íŒŒì¼ ë‹¤ìš´ë¡œë“œ URL
            save_path: ì €ìž¥í•  íŒŒì¼ ê²½ë¡œ (í™•ìž¥ìž ì œì™¸)
            
        Returns:
            (ì„±ê³µ ì—¬ë¶€, ì‹¤ì œ ì €ìž¥ëœ íŒŒì¼ ê²½ë¡œ)
        """
        try:
            response = requests.get(url, timeout=30, verify=False)
            response.raise_for_status()
            
            content = response.content
            
            # íŒŒì¼ íƒ€ìž… ê°ì§€
            file_type = self.detect_file_type(url, content)
            
            # ì ì ˆí•œ í™•ìž¥ìžë¡œ ì €ìž¥
            if file_type == 'pdf':
                final_path = f"{save_path}.pdf"
            elif file_type == 'hwp':
                final_path = f"{save_path}.hwp"
            elif file_type == 'docx':
                final_path = f"{save_path}.docx"
            elif file_type == 'txt':
                final_path = f"{save_path}.txt"
            else:
                # ì•Œ ìˆ˜ ì—†ëŠ” ê²½ìš° ì›ë³¸ URLì—ì„œ í™•ìž¥ìž ì¶”ì¶œ ì‹œë„
                ext = Path(url).suffix or '.bin'
                final_path = f"{save_path}{ext}"
            
            # íŒŒì¼ ì €ìž¥
            with open(final_path, 'wb') as f:
                f.write(content)
            
            return True, final_path
            
        except Exception as e:
            st.error(f"âŒ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url}\nì˜¤ë¥˜: {str(e)}")
            return False, ""
    
    def load_document_with_loader(self, file_path: str, url: str, doc_index: int) -> List[Document]:
        """
        íŒŒì¼ íƒ€ìž…ì— ë§žëŠ” LangChain ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë¡œë“œ
        
        Args:
            file_path: ë¡œì»¬ íŒŒì¼ ê²½ë¡œ
            url: ì›ë³¸ URL
            doc_index: ë¬¸ì„œ ì¸ë±ìŠ¤
            
        Returns:
            Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        file_ext = Path(file_path).suffix.lower()
        
        try:
            # PDF íŒŒì¼
            if file_ext == '.pdf':
                loader = PyPDFLoader(file_path)
                docs = loader.load()
                st.success(f"âœ… PDF ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(docs)} íŽ˜ì´ì§€")
            
            # HWP íŒŒì¼ - Windows íŠ¹í™” ì²˜ë¦¬
            elif file_ext in ['.hwp', '.hwpx']:
                st.info(f"ðŸ”„ HWP íŒŒì¼ ì²˜ë¦¬ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤)")
                
                try:
                    # olefileë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    import olefile
                    
                    if not olefile.isOleFile(file_path):
                        st.error("âŒ ì˜¬ë°”ë¥¸ HWP íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤.")
                        return []
                    
                    ole = olefile.OleFileIO(file_path)
                    
                    # HWP íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼ ì°¾ê¸°
                    text_content = ""
                    
                    # ë°©ë²• 1: PrvText ìŠ¤íŠ¸ë¦¼ (ë¯¸ë¦¬ë³´ê¸° í…ìŠ¤íŠ¸)
                    if ole.exists('PrvText'):
                        encoded_text = ole.openstream('PrvText').read()
                        
                        # ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
                        for encoding in ['utf-16', 'utf-16-le', 'cp949', 'euc-kr']:
                            try:
                                text_content = encoded_text.decode(encoding, errors='ignore')
                                if text_content.strip():
                                    break
                            except:
                                continue
                    
                    # ë°©ë²• 2: BodyText ìŠ¤íŠ¸ë¦¼ ì‹œë„
                    if not text_content.strip() and ole.exists('BodyText'):
                        try:
                            encoded_text = ole.openstream('BodyText').read()
                            text_content = encoded_text.decode('utf-16', errors='ignore')
                        except:
                            pass
                    
                    ole.close()
                    
                    if text_content.strip():
                        # Document ê°ì²´ ìƒì„±
                        doc = Document(
                            page_content=text_content,
                            metadata={'source': file_path}
                        )
                        docs = [doc]
                        st.success(f"âœ… HWP í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(text_content)} ê¸€ìž)")
                    else:
                        st.error("âŒ HWP íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        st.info("   ì´ HWP íŒŒì¼ì„ PDFë¡œ ë³€í™˜ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                        return []
                        
                except Exception as e:
                    st.error(f"âŒ HWP ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    st.info("   HWP íŒŒì¼ì„ PDFë¡œ ë³€í™˜í•˜ê±°ë‚˜, ë‹¤ë¥¸ ë¬¸ì„œë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
                    return []
            
            # DOCX íŒŒì¼
            elif file_ext in ['.docx', '.doc']:
                try:
                    # python-docx ì§ì ‘ ì‚¬ìš©
                    from docx import Document as DocxDocument
                    
                    docx = DocxDocument(file_path)
                    text_content = "\n".join([para.text for para in docx.paragraphs if para.text.strip()])
                    
                    if text_content.strip():
                        doc = Document(
                            page_content=text_content,
                            metadata={'source': file_path}
                        )
                        docs = [doc]
                        st.success(f"âœ… DOCX ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ ({len(text_content)} ê¸€ìž)")
                    else:
                        st.warning("âš ï¸ DOCX íŒŒì¼ì´ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                        return []
                except Exception as e:
                    st.warning(f"âŒ DOCX ì²˜ë¦¬ ì‹¤íŒ¨, ê±´ë„ˆëœë‹ˆë‹¤: {str(e)}")
                    return []
            
            # TXT íŒŒì¼
            elif file_ext == '.txt':
                encodings = ['utf-8', 'cp949', 'euc-kr', 'utf-8-sig']
                docs = None
                for encoding in encodings:
                    try:
                        loader = TextLoader(file_path, encoding=encoding)
                        docs = loader.load()
                        break
                    except:
                        continue
                
                if docs:
                    st.success(f"âœ… TXT ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
                else:
                    st.error("âŒ TXT íŒŒì¼ ì¸ì½”ë”© ê°ì§€ ì‹¤íŒ¨")
                    return []
            
            # ê¸°íƒ€ íŒŒì¼
            else:
                try:
                    loader = UnstructuredFileLoader(file_path, mode="single")
                    docs = loader.load()
                    st.success(f"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ ({file_ext})")
                except:
                    st.error(f"âŒ {file_ext} íŒŒì¼ í˜•ì‹ì€ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    return []
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for doc in docs:
                doc.metadata['source_url'] = url
                doc.metadata['doc_index'] = doc_index
                doc.metadata['file_type'] = file_ext
            
            return docs
            
        except Exception as e:
            st.error(f"âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {file_path}\nì˜¤ë¥˜: {str(e)}")
            return []
    
    def load_documents_from_urls(self, file_urls: List[str]) -> List[Document]:
        all_documents = []
        temp_dir = tempfile.mkdtemp()
        
        # ë¹ˆ URL í•„í„°ë§ ê°•í™”
        valid_urls = []
        for url in file_urls:
            if url and isinstance(url, str) and url.strip():
                url_clean = url.strip()
                if url_clean.lower() not in ['nan', 'none', 'null', '']:
                    valid_urls.append(url_clean)
        
        if not valid_urls:
            st.warning("âš ï¸ ìœ íš¨í•œ ë¬¸ì„œ URLì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        st.info(f"   ì´ {len(valid_urls)}ê°œì˜ ë¬¸ì„œë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
        
        success_count = 0
        fail_count = 0
        
        for idx, url in enumerate(valid_urls):
            try:
                st.info(f"   ë¬¸ì„œ {idx+1}/{len(valid_urls)} ì²˜ë¦¬ ì¤‘...")
                
                temp_file_path = os.path.join(temp_dir, f"doc_{idx}")
                
                # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                success, actual_path = self.download_file(url, temp_file_path)
                
                if success and actual_path:
                    # ë¬¸ì„œ ë¡œë“œ
                    docs = self.load_document_with_loader(actual_path, url, idx)
                    
                    if docs and len(docs) > 0:
                        # ë¹ˆ ë¬¸ì„œ ì²´í¬
                        valid_docs = [d for d in docs if d.page_content.strip()]
                        if valid_docs:
                            all_documents.extend(valid_docs)
                            success_count += 1
                        else:
                            st.warning(f"âš ï¸ ë¬¸ì„œ {idx+1}: ë‚´ìš©ì´ ë¹„ì–´ìžˆì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                            fail_count += 1
                    else:
                        fail_count += 1
                    
                    # ìž„ì‹œ íŒŒì¼ ì‚­ì œ
                    try:
                        if os.path.exists(actual_path):
                            os.remove(actual_path)
                    except:
                        pass
                else:
                    fail_count += 1
            except Exception as e:
                st.error(f"âŒ ë¬¸ì„œ {idx+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                fail_count += 1
                continue
        
        # ê²°ê³¼ ìš”ì•½
        if all_documents:
            st.success(f"ðŸŽ‰ ì„±ê³µ: {success_count}ê°œ ë¬¸ì„œ, ì´ {len(all_documents)}ê°œ ì²­í¬ ë¡œë“œ!")
            if fail_count > 0:
                st.warning(f"âš ï¸ ê±´ë„ˆëœ€/ì‹¤íŒ¨: {fail_count}ê°œ ë¬¸ì„œ")
        else:
            st.error("âŒ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        return all_documents
    
    def build_vectorstore(self, documents: List[Document], chunk_size: int = 1000, chunk_overlap: int = 200):
        if not documents:
            st.error("âŒ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
        )
        
        splits = text_splitter.split_documents(documents)
        st.info(f"   ì´ {len(splits)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±ë¨")
        
        # ë””ë²„ê¹…: ì„¤ì • í™•ì¸
        st.info(f"ðŸ” Embedding Base URL: {self.embedding_base_url}")
        st.info(f"ðŸ” API Key ì•ž 10ìž: {self.api_key[:5]}...")
        
        try:
            # í…ŒìŠ¤íŠ¸ ìž„ë² ë”©
            st.info("ðŸ§ª ìž„ë² ë”© í…ŒìŠ¤íŠ¸ ì¤‘...")
            test_embedding = self.embeddings.embed_query("í…ŒìŠ¤íŠ¸")
            st.success(f"âœ… ìž„ë² ë”© í…ŒìŠ¤íŠ¸ ì„±ê³µ! (ì°¨ì›: {len(test_embedding)})")
            
            # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            st.info("ðŸ”„ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
            self.vectorstore = FAISS.from_documents(splits, self.embeddings)
            
            st.success("âœ… ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ!")
            
        except Exception as e:
            st.error(f"âŒ ë²¡í„° DB êµ¬ì¶• ì‹¤íŒ¨: {str(e)}")
            st.error(f"ðŸ” ì‚¬ìš© ì¤‘ì¸ Embedding URL: {self.embedding_base_url}")
            
            # ìƒì„¸ ì˜¤ë¥˜ ì¶œë ¥
            import traceback
            with st.expander("ìƒì„¸ ì˜¤ë¥˜ ë³´ê¸°"):
                st.code(traceback.format_exc())
    
    # â­ ì‹ ê·œ ë©”ì„œë“œ: LLM ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì§ˆë¬¸ ìƒì„±
    def generate_smart_questions(self, num_questions: int = 4) -> List[str]:
        """
        LLMì„ ì‚¬ìš©í•´ ë¬¸ì„œ ë‚´ìš© ê¸°ë°˜ ì§ˆë¬¸ ìžë™ ìƒì„±
        
        Args:
            num_questions: ìƒì„±í•  ì§ˆë¬¸ ê°œìˆ˜ (ê¸°ë³¸ 4ê°œ)
            
        Returns:
            ìƒì„±ëœ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        if not self.vectorstore:
            return [
                "ì´ ì‚¬ì—…ì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
                "ì§€ì› ëŒ€ìƒì€ ëˆ„êµ¬ì¸ê°€ìš”?",
            ]
        
        try:
            # 1. ë¬¸ì„œ ìƒ˜í”Œ ì¶”ì¶œ (ë‹¤ì–‘í•œ ë¶€ë¶„ì—ì„œ)
            sample_docs = self.vectorstore.similarity_search("", k=5)
            
            # 2. ìƒ˜í”Œ í…ìŠ¤íŠ¸ êµ¬ì„± (ê° ë¬¸ì„œ ì•žë¶€ë¶„ 500ìž)
            sample_texts = []
            for i, doc in enumerate(sample_docs[:3], 1):  # ìµœëŒ€ 3ê°œ ë¬¸ì„œ
                content_preview = doc.page_content[:500].strip()
                sample_texts.append(f"[ë¬¸ì„œ {i}]\n{content_preview}")
            
            combined_sample = "\n\n".join(sample_texts)
            
            # 3. LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = f"""ë‹¹ì‹ ì€ ì •ë¶€ì§€ì›ì‚¬ì—… ê³µê³  ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
ì•„ëž˜ ë¬¸ì„œ ì¼ë¶€ë¥¼ ì½ê³ , ì‚¬ìš©ìžê°€ ì´ ë¬¸ì„œì— ëŒ€í•´ ê°€ìž¥ ê¶ê¸ˆí•´í•  ë§Œí•œ ì§ˆë¬¸ {num_questions}ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.

[ë¬¸ì„œ ë‚´ìš© ìƒ˜í”Œ]
{combined_sample}

[ì¤‘ìš” ì§€ì¹¨]
1. ë¬¸ì„œì— ë‹µì´ ëª…í™•ížˆ ì¡´ìž¬í•˜ëŠ” ì§ˆë¬¸ë§Œ ìƒì„±í•˜ì„¸ìš”
2. ê° ì§ˆë¬¸ì€ í•œ ë¬¸ìž¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìž‘ì„±í•˜ì„¸ìš”
3. ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ì–´ì•¼ í•©ë‹ˆë‹¤
4. ë‹¤ìŒ ì£¼ì œë¥¼ ìš°ì„ ì ìœ¼ë¡œ ë‹¤ë£¨ì„¸ìš”:
   - ì§€ì› ëŒ€ìƒ ë° ìžê²©
   - ì‹ ì²­ ë°©ë²• ë° ì ˆì°¨
   - ì§€ì› ë‚´ìš© ë° ê·œëª¨
   - ì œì¶œ ì„œë¥˜
   - í‰ê°€ ê¸°ì¤€
   - ì‚¬ì—… ê¸°ê°„

[ì¶œë ¥ í˜•ì‹]
1. ì§ˆë¬¸1
2. ì§ˆë¬¸2
3. ì§ˆë¬¸3
4. ì§ˆë¬¸4

ìœ„ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì¶”ê°€ ì„¤ëª… ì—†ì´ ì§ˆë¬¸ë§Œ ë‚˜ì—´í•˜ì„¸ìš”."""

            # 4. LLM í˜¸ì¶œ
            response = self.llm.invoke(prompt)
            questions_text = response.content if hasattr(response, "content") else str(response)
            
            # 5. íŒŒì‹±
            questions = []
            for line in questions_text.split("\n"):
                line = line.strip()
                # "1. " ë˜ëŠ” "- " í˜•ì‹ íŒŒì‹±
                if line and (line[0].isdigit() or line.startswith("-")):
                    # ë²ˆí˜¸/ê¸°í˜¸ ì œê±°
                    if "." in line:
                        clean_q = line.split(".", 1)[-1].strip()
                    else:
                        clean_q = line.lstrip("- ").strip()
                    
                    if clean_q and len(clean_q) > 5:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                        questions.append(clean_q)
            
            # 6. ê²°ê³¼ ê²€ì¦
            if len(questions) >= 2:
                return questions[:num_questions]
            else:
                # Fallback: ê¸°ë³¸ ì§ˆë¬¸
                return [
                    "ì´ ì‚¬ì—…ì˜ ì£¼ìš” ëª©ì ê³¼ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
                    "ì§€ì› ëŒ€ìƒê³¼ ì‹ ì²­ ìžê²©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì§€ì›ì„ ë°›ì„ ìˆ˜ ìžˆë‚˜ìš”?",
                    "ì‹ ì²­ ì‹œ ì œì¶œí•´ì•¼ í•  ì„œë¥˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                ][:num_questions]
                
        except Exception as e:
            print(f"âš ï¸ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            # Fallback: ê¸°ë³¸ ì§ˆë¬¸
            return [
                "ì´ ì‚¬ì—…ì˜ í•µì‹¬ ë‚´ìš©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "ëˆ„ê°€ ì´ ì‚¬ì—…ì— ì§€ì›í•  ìˆ˜ ìžˆë‚˜ìš”?",
                "ì§€ì› ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                "ì–´ë–¤ í˜œíƒì„ ë°›ì„ ìˆ˜ ìžˆë‚˜ìš”?",
            ][:num_questions]
    
    def query(self, question: str, use_enhanced_prompt: bool = True) -> Tuple[str, List[Document]]:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (ê°œì„ ëœ ë²„ì „)
        
        Args:
            question: ì‚¬ìš©ìž ì§ˆë¬¸
            use_enhanced_prompt: í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            (ë‹µë³€, ì°¸ì¡° ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸)
        """
        if not self.vectorstore:
            return "âŒ ë²¡í„° DBê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.", []
        
        try:
            # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (Top-5)
            source_docs = self.vectorstore.similarity_search(question, k=5)
            
            if not source_docs:
                return "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.", []
            
            # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = "\n\n---\n\n".join([
                f"[ë¬¸ì„œ {i+1}]\n{doc.page_content}" 
                for i, doc in enumerate(source_docs)
            ])
            
            # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
            if use_enhanced_prompt:
                prompt = f"""ë‹¹ì‹ ì€ ì •ë¶€ì§€ì›ì‚¬ì—… ê³µê³  ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. 
ì•„ëž˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì£¼ìš” ì§€ì¹¨]
1. ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ì •í™•í•œ ìˆ˜ì¹˜, ë‚ ì§œ, ì¡°ê±´ ë“±ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ì„¸ìš”
3. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì— ëª…ì‹œë˜ì–´ ìžˆì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”
4. ë‹µë³€ì€ êµ¬ì¡°í™”ë˜ê³  ì½ê¸° ì‰½ê²Œ ìž‘ì„±í•˜ì„¸ìš” (í•„ìš”ì‹œ ê¸€ë¨¸ë¦¬ê¸°í˜¸ ì‚¬ìš©)
5. ì „ë¬¸ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”
6. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ìžì—°ìŠ¤ëŸ½ê²Œ ìž‘ì„±í•˜ì„¸ìš”

[ì°¸ì¡° ë¬¸ì„œ ë‚´ìš©]
{context}

[ì‚¬ìš©ìž ì§ˆë¬¸]
{question}

[ë‹µë³€]"""
            else:
                prompt = f"""ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
            
            # 4. LLM í˜¸ì¶œ (temperatureëŠ” ì´ë¯¸ ì´ˆê¸°í™” ì‹œ ì„¤ì •ë¨)
            response = self.llm.invoke(prompt)
            answer = response.content if hasattr(response, "content") else str(response)
            
            return answer, source_docs
            
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\n{error_detail}", []
    
    def similarity_search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        """
        ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
            
        Returns:
            (Document, ìœ ì‚¬ë„ ì ìˆ˜) ë¦¬ìŠ¤íŠ¸
        """
        if not self.vectorstore:
            return []
        
        try:
            results = self.vectorstore.similarity_search_with_score(query, k=k)
            return results
        except Exception as e:
            st.error(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []


def extract_file_urls(project_row) -> List[str]:
    """
    í”„ë¡œì íŠ¸ rowì—ì„œ ê³µê³ ê·œê²©ì„œURL1~10 ì¶”ì¶œ
    
    Args:
        project_row: ì‚¬ì—…ê³µê³  DataFrame row
        
    Returns:
        ìœ íš¨í•œ íŒŒì¼ URL ë¦¬ìŠ¤íŠ¸ (PDF, HWP ë“±)
    """
    file_urls = []
    
    for i in range(1, 11):
        col_name = f"ê³µê³ ê·œê²©ì„œURL{i}"
        if col_name in project_row.index:
            url = project_row[col_name]
            if url and str(url).strip() and str(url) != 'nan':
                file_urls.append(str(url).strip())
    
    return file_urls


@st.cache_resource
def get_rag_system(api_key: str, llm_base_url: str, embedding_base_url: str = None):
    """
    RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìºì‹±)
    """
    return ProjectRAGSystem(api_key, llm_base_url, embedding_base_url)


def display_source_documents(source_docs: List[Document]):
    """
    ì°¸ì¡° ë¬¸ì„œ í‘œì‹œ (Streamlit UI)
    
    Args:
        source_docs: ì°¸ì¡°ëœ Document ë¦¬ìŠ¤íŠ¸
    """
    if not source_docs:
        return
    
    st.markdown("### ðŸ“š ì°¸ì¡° ë¬¸ì„œ")
    
    for idx, doc in enumerate(source_docs, 1):
        with st.expander(f"ì°¸ì¡° {idx}: {doc.metadata.get('source_url', 'Unknown')[:250]}..."):
            st.markdown(f"**ì¶œì²˜:** {doc.metadata.get('source_url', 'N/A')}")
            st.markdown(f"**íŒŒì¼ íƒ€ìž…:** {doc.metadata.get('file_type', 'N/A')}")
            st.markdown(f"**íŽ˜ì´ì§€:** {doc.metadata.get('page', 'N/A')}")
            st.markdown("**ë‚´ìš©:**")
            content = doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content
            st.text(content)